<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Sangyu Han</title>

    <meta name="author" content="Sangyu Han">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Sangyu Han
                </p>
                I am currently a graduate student in the Interdisciplinary Program in Artificial Intelligence at <a href="https://www.snu.ac.kr/">Seoul National University</a>,
                 advised by Professor  <a href="https://scholar.google.com/citations?user=h_8-1M0AAAAJ&hl=en">Nojun Kwak</a> in the <a href="https://mipal.snu.ac.kr/mipal"></a>Machine Intelligence and Pattern Analysis Lab (MIPAL)</a>. 
                Prior to starting my graduate studies, I earned my first bachelor’s degree from the School of Health and Environment Science and my second bachelor’s degree from the Interdisciplinary Major in Artificial Intelligence at <a href="https://www.korea.edu/sites/en/index.do"> Korea University</a>.
                <p style="text-align:center">
                  <a href="mailto:acoexist96@snu.ac.kr">Email</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=Xe9KzXgAAAAJ&hl">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Sangyu-Han/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/Sangyu_Han.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Sangyu_Han.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am deeply interested in Explainable AI, particularly in Mechanistic Interpretability(i.e., reverse-engineering neural networks). 
                  I aim to identify monosemantic representations within a model’s hidden layers and uncover how these representations interact throughout the model. 
                  My goal is to enable anyone to easily observe a model’s internal representation processes, thereby contributing to the development of safe, transparent, and trustworthy AI.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



    

    <tr onmouseout="GIG_stop()" onmouseover="GIG_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/GIG.png' width=100%>
        </div>
        <script type="text/javascript">
          function ever_start() {
            document.getElementById('GIG_image').style.opacity = "1";
          }

          function ever_stop() {
            document.getElementById('GIG_image').style.opacity = "0";
          }
          ever_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
			<span class="papertitle">DECOMPOSE THE MODEL: MECHANISTIC INTERPRETABILITY IN IMAGE MODELS WITH GENERALIZED
        INTEGRATED GRADIENTS (GIG)        
      </span>
        </a>
        <br>
				<a href="https://scholar.google.com.hk/citations?user=nS8vgAQAAAAJ&hl=it">Yearim Kim*</a>, 
				<a href="https://scholar.google.com/citations?user=Xe9KzXgAAAAJ&hl"><strong>Sangyu Han*</strong></a>,
				<a href="https://scholar.google.com/citations?user=NcRvNcsAAAAJ&hl">Sangbum Han</a>,
        <a href="https://scholar.google.com/citations?user=h_8-1M0AAAAJ&hl">Nojun Kwak</a>.
				<br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://gig2025iclr.netlify.app/graph_visualization_additional.html">project page</a>
        /
        <a href="https://arxiv.org/abs/2411.18645">arXiv</a>
        <p></p>
        <p>
          We demonstrated that attributions between monosemantic concept vectors can reveal the development of complex concepts (for example, the concept of a "dog’s face") throughout the hidden layers of an image model. 
          Our method is computationally efficient and robust to noise, enabling it to show how adversarial noise alters a model’s predictions.

        </p>
      </td>
    </tr>


    <tr onmouseout="BiICE_stop()" onmouseover="BiICE_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/Bi-ICE.png' width=100%>
        </div>
        <script type="text/javascript">
          function ever_start() {
            document.getElementById('BiICE_image').style.opacity = "1";
          }

          function ever_stop() {
            document.getElementById('BiICE_image').style.opacity = "0";
          }
          ever_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
			<span class="papertitle">Bi-ICE: An Inner Interpretable Framework for Image Classification via Bi-directional Interactions between Concept and Input Embeddings       
      </span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=JaQMvXsAAAAJ&hl">Jinyung Hong*</a>,
				<a href="https://scholar.google.com.hk/citations?user=nS8vgAQAAAAJ&hl=it">Yearim Kim*</a>, 
        <a href="https://scholar.google.com/citations?user=-QseKLUAAAAJ&hl=">Keun Hee Park</a>,
				<a href="https://scholar.google.com/citations?user=Xe9KzXgAAAAJ&hl"><strong>Sangyu Han</strong></a>,
        <a href="https://scholar.google.com/citations?user=h_8-1M0AAAAJ&hl">Nojun Kwak</a>,
        <a href="https://scholar.google.com/citations?user=UAmGg-cAAAAJ&hl">Theodore P Pavlic</a>,
				<br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2411.18645">arXiv</a>
        <p></p>
        <p>
          We propose a conceptual framework and a bi-directional interpretation module (Bi-ICE) for large-scale image classification tasks that uncovers internal mechanisms at multiple levels. Using human-interpretable concepts, 
          our approach generates predictions, quantifies concept contributions, and localizes them in the inputs. This method not only increases transparency and interpretability but also reveals how concepts are learned and converge within the model.

        </p>
      </td>
    </tr>


    <tr onmouseout="SRD_stop()" onmouseover="SRD_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='SRD_image'><img src='images/SRD.png'></div>
          <div class="two" id='SRD_gif'><img src='images/SRD.gif'></div>
        </div>
        <script type="text/javascript">
          function SRD_start() {
            document.getElementById('SRD_image').style.opacity = "1";
            document.getElementById('SRD_gif').style.opacity = "0"; // SRD.gif 투명화
          }
    
          function SRD_stop() {
            document.getElementById('SRD_image').style.opacity = "0";
            document.getElementById('SRD_gif').style.opacity = "1"; // SRD.gif 원래 상태로
          }
          SRD_stop();
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <span class="papertitle">Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition      
        </span>
        <br>
        <a href="https://scholar.google.com/citations?user=Xe9KzXgAAAAJ&hl"><strong>Sangyu Han*</strong></a>,
        <a href="https://scholar.google.com.hk/citations?user=nS8vgAQAAAAJ&hl=it">Yearim Kim*</a>, 
        <a href="https://scholar.google.com/citations?user=h_8-1M0AAAAJ&hl">Nojun Kwak</a>.
        <br>
        <em>ICLR</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2402.03348">arXiv</a>
        <p></p>
        <p>
          We propose SRD (Sharing Ratio Decomposition), a robust XAI method that faithfully reflects a model's inference process by analyzing nonlinear interactions between filters from a vector perspective. 
          Additionally, we introduce APOP (Activation-Pattern-Only Prediction) to redefine relevance, incorporating both active and inactive neurons. 
          SRD enables recursive decomposition of feature vectors, providing high-resolution receptive fields and enhancing interpretability and robustness in model explanations.
        </p>
      </td>
    </tr>
    


    
   
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This page is a fork of <a href="https://jonbarron.info/">Jon Barron's webpage</a>. Thanks to Jon Barron for sharing!
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
